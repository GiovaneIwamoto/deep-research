{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain langgraph tavily-python langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get research topic from user\n",
    "user_input = input(\"Enter the research topic or question: \").strip()\n",
    "\n",
    "# Ask if user wants to enable reflection\n",
    "reflection_choice = input(\"Enable reflection? (y/n): \").strip().lower()\n",
    "if reflection_choice in [\"y\", \"yes\", \"true\"]:\n",
    "    use_reflection = True\n",
    "    while True:\n",
    "        try:\n",
    "            reflection_loops = int(input(\"How many reflection loops?: \").strip())\n",
    "            if reflection_loops >= 1:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter a number greater than or equal to one.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid integer.\")\n",
    "else:\n",
    "    use_reflection = False\n",
    "    reflection_loops = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging for the notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(levelname)s] %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "class QueryResult(BaseModel):\n",
    "    title: str = None\n",
    "    url: str = None\n",
    "    resume: str = None\n",
    "\n",
    "class ReportState(BaseModel):\n",
    "    user_input: str = None\n",
    "    final_response: str = None\n",
    "    search_queries: List[str] = []\n",
    "    queries_results: Annotated[List[QueryResult], operator.add]\n",
    "    research_loop_count: int = 0\n",
    "    use_reflection: bool = True\n",
    "    reflection_loops: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "reasoning_llm_openai = ChatOpenAI(\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    "    reasoning_effort=\"medium\",\n",
    ")\n",
    "\n",
    "default_llm_openai = ChatOpenAI(model=\"gpt-4.1-mini-2025-04-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = \"\"\"\n",
    "You are a research planner agent.\n",
    "You are working on a project that aims to answer user's question using resources found online.\n",
    "\n",
    "Your asnwer should be technical, detailed and well structured using up to date information.\n",
    "Cite facts, data and specific informations.\n",
    "\n",
    "Here is the user input:\n",
    "\n",
    "<USER_INPUT>\n",
    "{user_input}\n",
    "</USER_INPUT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_queries = agent_prompt + \"\"\"\n",
    "Your first objective is to build a list of queries that\n",
    "will be used to find answers to the user's question.\n",
    "\n",
    "Build only 2 queries, each query should be a single sentence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_search = agent_prompt + \"\"\"\n",
    "Your objective here is to analyze the web search results and make a synthesis of it.\n",
    "Emphasize the most relevant information based on user's question.\n",
    "\n",
    "After your work, another agent will use the synthesis to build a final response to the user,\n",
    "so make sure the synthesis contains only useful information.\n",
    "\n",
    "Write about maximum of two paragraphs, and use only the information obtained from the web search results.\n",
    "\n",
    "Here is the web search results:\n",
    "\n",
    "<SEARCH_RESULTS>\n",
    "{search_results}\n",
    "</SEARCH_RESULTS>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_final_response = agent_prompt + \"\"\"\n",
    "Your objective here is to develop a final response to the user using the reports made during\n",
    "the web search, with their syntesis.\n",
    "\n",
    "The response should contain something between 3 and 5 paragraphs.\n",
    "\n",
    "Here is the web search results:\n",
    "\n",
    "<SEARCH_RESULTS>\n",
    "{search_results}\n",
    "</SEARCH_RESULTS>\n",
    "\n",
    "You must add reference citations with the number of the citation (e.g. [1], [2], etc.) at the end of each paragraph.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_instructions = \"\"\"You are an expert research assistant analyzing a summary about {research_topic}.\n",
    "\n",
    "<GOAL>\n",
    "1. Review the current summary and identify knowledge gaps or aspects that need further clarification or exploration.\n",
    "2. Generate 1â€“2 follow-up search questions that could help fill these gaps.\n",
    "3. The questions should be specific enough to return relevant search results, but not overly narrow or technical.\n",
    "4. Avoid overly broad or vague queries as well as too niche or academic questions unlikely to return good results on the web.\n",
    "</GOAL>\n",
    "\n",
    "<REQUIREMENTS>\n",
    "- Each question should be short, clear, and self-contained.\n",
    "- Prefer general-purpose terminology that search engines can easily understand.\n",
    "</REQUIREMENTS>\n",
    "\n",
    "<FORMAT>\n",
    "Format your response as a JSON object with these exact keys:\n",
    "- knowledge_gaps: List of descriptions of what information is missing or needs clarification\n",
    "- follow_up_queries: List of 1-2 specific questions to address these gaps\n",
    "</FORMAT>\n",
    "\n",
    "<TASK>\n",
    "Reflect carefully on the Summary to identify knowledge gaps and produce 1-2 follow-up queries. Then, produce your output following this JSON format:\n",
    "{{\n",
    "    \"knowledge_gaps\": [\n",
    "        \"The summary lacks information about performance metrics and benchmarks\",\n",
    "        \"Missing details about implementation challenges\"\n",
    "    ],\n",
    "    \"follow_up_queries\": [\n",
    "        \"What are typical performance benchmarks and metrics used to evaluate [specific technology]?\",\n",
    "        \"What are the main implementation challenges when deploying [specific technology]?\"\n",
    "    ]\n",
    "}}\n",
    "</TASK>\n",
    "\n",
    "Provide your analysis in JSON format:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_first_queries(state: ReportState):\n",
    "    class QueryList(BaseModel):\n",
    "        queries: List[str]\n",
    "    \n",
    "    user_input = state.user_input\n",
    "    prompt = build_queries.format(user_input=user_input)\n",
    "    query_llm = default_llm_openai.with_structured_output(QueryList)\n",
    "\n",
    "    result = query_llm.invoke(prompt)\n",
    "\n",
    "    logging.info(\"Generated search queries:\")\n",
    "    logging.info(result)\n",
    "    logging.info(\"End of query generation.\")\n",
    "\n",
    "    return{\"search_queries\": result.queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "def spawn_researchers(state: ReportState):\n",
    "    logging.info(\"Spawning parallel researcher agents.\")\n",
    "    return [Send(\"single_search\", query) for query in state.search_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "def single_search(query: str):\n",
    "    tavily_client = TavilyClient()\n",
    "    results = tavily_client.search(query, \n",
    "                                   max_results=1,\n",
    "                                   include_raw_content=False)\n",
    "    \n",
    "    url = results[\"results\"][0][\"url\"]\n",
    "    url_extraction = tavily_client.extract(url)\n",
    "\n",
    "    if len(url_extraction[\"results\"]) > 0:\n",
    "        raw_content = url_extraction[\"results\"][0][\"raw_content\"]\n",
    "        prompt = resume_search.format(user_input=user_input, search_results=raw_content)\n",
    "        llm_result = default_llm_openai.invoke(prompt)\n",
    "        query_results = QueryResult(\n",
    "            title=results[\"results\"][0][\"title\"],\n",
    "            url=url,\n",
    "            resume=llm_result.content\n",
    "        )\n",
    "        logging.info(f\"Query: {query}\")\n",
    "        logging.info(\"Search result summary:\")\n",
    "        logging.info(query_results)\n",
    "    else:\n",
    "        query_results = QueryResult(\n",
    "            title=results[\"results\"][0][\"title\"],\n",
    "            url=url,\n",
    "            resume=\"No content could be extracted from the provided URL.\"\n",
    "        )\n",
    "        logging.warning(f\"No content extracted for query: {query} (URL: {url})\")\n",
    "    return {\"queries_results\": [query_results]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_writer(state: ReportState):\n",
    "    search_results = \"\"\n",
    "    reference = \"\"\n",
    "    for i, result in enumerate(state.queries_results):\n",
    "        search_results += f\"[{i+1}]\\n\\n\"\n",
    "        search_results += f\"Title: {result.title}\\n\"\n",
    "        search_results += f\"URL: {result.url}\\n\"\n",
    "        search_results += f\"Content: {result.resume}\\n\\n\"\n",
    "\n",
    "        reference += f\"[{i+1}] - {result.title} ({result.url})\\n\"\n",
    "    \n",
    "    prompt = build_final_response.format(user_input=state.user_input, search_results=search_results)\n",
    "    \n",
    "    logging.info(\"Compiled search results for final synthesis:\")\n",
    "    logging.info(search_results)\n",
    "    logging.info(\"End of search results compilation.\\n\")\n",
    "\n",
    "    llm_result = default_llm_openai.invoke(prompt)\n",
    "\n",
    "    final_response = llm_result + \"\\n\\nReferences:\\n\" + reference\n",
    "\n",
    "    return {\"final_response\": final_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "# Updated reflect_on_summary function to handle multiple queries\n",
    "def reflect_on_summary(state: ReportState):\n",
    "    new_loop_count = state.research_loop_count + 1\n",
    "\n",
    "    result = reasoning_llm_openai.invoke(\n",
    "        [SystemMessage(content=reflection_instructions.format(research_topic=state.user_input)),\n",
    "        HumanMessage(content=f\"Reflect on our existing knowledge: \\n === \\n {state.queries_results}, \\n === \\n And now identify knowledge gaps and generate 1-2 follow-up web search queries:\")]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Try to parse as JSON first\n",
    "        reflection_content = json.loads(result.content)\n",
    "\n",
    "        logging.info(\"Reflection analysis:\")\n",
    "        logging.info(f\"Knowledge gaps: {reflection_content.get('knowledge_gaps', [])}\")\n",
    "        logging.info(f\"Follow-up queries: {reflection_content.get('follow_up_queries', [])}\")\n",
    "        logging.info(\"End of reflection analysis.\\n\")\n",
    "\n",
    "        # Get the follow-up queries\n",
    "        queries = reflection_content.get('follow_up_queries', [])\n",
    "\n",
    "        # Check if queries is None or empty\n",
    "        if not queries:\n",
    "            queries = [f\"Tell me more about {state.user_input}\"]\n",
    "\n",
    "        return {\n",
    "            \"search_queries\": queries,\n",
    "            \"research_loop_count\": new_loop_count\n",
    "        }\n",
    "    \n",
    "    except (json.JSONDecodeError, KeyError, AttributeError):\n",
    "        fallback_query = f\"Tell me more about {state.user_input}\"\n",
    "        return {\n",
    "            \"search_queries\": [fallback_query],\n",
    "            \"research_loop_count\": new_loop_count\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_single_search(state: ReportState):\n",
    "    if state.use_reflection and state.research_loop_count < state.reflection_loops:\n",
    "        return \"reflect_on_summary\"\n",
    "    else:\n",
    "        return \"final_writer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "builder = StateGraph(ReportState)\n",
    "builder.add_node(\"build_first_queries\", build_first_queries)\n",
    "builder.add_node(\"single_search\", single_search)\n",
    "builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
    "builder.add_node(\"final_writer\", final_writer)\n",
    "\n",
    "builder.add_edge(START, \"build_first_queries\")\n",
    "builder.add_conditional_edges(\"build_first_queries\", spawn_researchers, [\"single_search\"])\n",
    "builder.add_conditional_edges(\"single_search\", route_after_single_search, [\"reflect_on_summary\", \"final_writer\"])\n",
    "builder.add_conditional_edges(\"reflect_on_summary\", spawn_researchers, [\"single_search\"])\n",
    "builder.add_edge(\"final_writer\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\n",
    "    \"user_input\": user_input,\n",
    "    \"use_reflection\": use_reflection,\n",
    "    \"reflection_loops\": reflection_loops\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['final_response'].messages[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
