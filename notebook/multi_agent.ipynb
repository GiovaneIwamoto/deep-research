{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain langgraph tavily-python langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Write a report on the impact of climate change on polar bears.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "class QueryResult(BaseModel):\n",
    "    title: str = None\n",
    "    url: str = None\n",
    "    resume: str = None\n",
    "    \n",
    "\n",
    "class ReportState(BaseModel):\n",
    "    user_input: str = None\n",
    "    final_response: str = None\n",
    "    queries: List[str] = []\n",
    "    queries_results: Annotated[List[QueryResult], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = {\n",
    "    \"effort\": \"medium\",  # 'low', 'medium', or 'high'\n",
    "    \"summary\": \"auto\",  # 'detailed', 'auto', or None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "reasoning_llm_deepseek = ChatOllama(\n",
    "    model = \"deepseek-r1:1.5b\",\n",
    ")\n",
    "\n",
    "reasoning_llm_openai = ChatOpenAI(\n",
    "    model=\"o4-mini\", reasoning=reasoning\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = \"\"\"\n",
    "You are a research planner agent\n",
    "You are  working on a project that aims to answer user's question\n",
    "using resources found online\n",
    "\n",
    "Your asnwer should be technical, detailed and well structured using up to date information\n",
    "Cite facts, data and specific informations.\n",
    "\n",
    "Here is the user input\n",
    "<USER_INPUT>\n",
    "{user_input}\n",
    "</USER_INPUT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_queries = agent_prompt + \"\"\"\n",
    "Your first objective is to build a list of queries that\n",
    "will be used to find answers to the user's question.\n",
    "\n",
    "Answer with anything between 2 and 3 queries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_search = agent_prompt + \"\"\"\n",
    "Your objective here is to analyze the web search results and make a synthesis of it.\n",
    "Emphasize the most relevant information based on user's question.\n",
    "\n",
    "After your work, another agent will use the synthesis to build a final response to the user,\n",
    "so make sure the synthesis contains only useful information.\n",
    "\n",
    "Be concise and clear, do not preamble.\n",
    "\n",
    "Here is the web search results:\n",
    "<SEARCH_RESULTS>\n",
    "{search_results}\n",
    "</SEARCH_RESULTS>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_final_response = agent_prompt + \"\"\"\n",
    "Your objective here is to develop a final response to the user using the reports made during\n",
    "the web search, with their syntesis.\n",
    "\n",
    "The response should contain something between 3 and 5 paragraphs.\n",
    "\n",
    "Here is the web search results:\n",
    "<SEARCH_RESULTS>\n",
    "{search_results}\n",
    "</SEARCH_RESULTS>\n",
    "\n",
    "You must add reference citations with the number of the citation (e.g. [1], [2], etc.) at the end of each paragraph.\n",
    "and the articles you used in each paragraph of your answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_first_queries(state: ReportState):\n",
    "    class QueryList(BaseModel):\n",
    "        queries: List[str]\n",
    "    \n",
    "    user_input = state.user_input\n",
    "    prompt = build_queries.format(user_input=user_input)\n",
    "    query_llm = llm.with_structured_output(QueryList)\n",
    "\n",
    "    result = query_llm.invoke(prompt)\n",
    "    \n",
    "    print(\"Generated queries from Query Builder: \\n {result}\\n\", result)\n",
    "\n",
    "    return{\"queries\": result.queries} #Formatted queries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "def spawn_researchers(state: ReportState):\n",
    "    return [Send(\"single_search\", query) for query in state.queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "#debug this\n",
    "def single_search(query: str):\n",
    "    tavily_client = TavilyClient()\n",
    "    results = tavily_client.search(query, \n",
    "                                   max_results=1,\n",
    "                                   include_raw_content=False)\n",
    "    \n",
    "    url = results[\"results\"][0][\"url\"]\n",
    "    url_extraction = tavily_client.extract(url)\n",
    "\n",
    "    if len(url_extraction[\"results\"]) > 0:\n",
    "        raw_content = url_extraction[\"results\"][0][\"raw_content\"]\n",
    "        prompt = resume_search.format(user_input=user_input, search_results=raw_content)\n",
    "        llm_result = llm.invoke(prompt)\n",
    "\n",
    "        query_results = QueryResult(title=results[\"results\"][0][\"title\"],\n",
    "                                   url=url,\n",
    "                                   resume=llm_result.content)\n",
    "        \n",
    "        print(\"\\n\\nAgent for query:\", query)\n",
    "        print(\"Result:\", query_results)\n",
    "        \n",
    "        return{\"queries_results\": [query_results]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_writer(state: ReportState):\n",
    "    search_results = \"\"\n",
    "    reference = \"\"\n",
    "    for i, result in enumerate(state.queries_results):\n",
    "        search_results += f\"[{i+1}]\\n\\n\"\n",
    "        search_results += f\"Title: {result.title}\\n\"\n",
    "        search_results += f\"URL: {result.url}\\n\"\n",
    "        search_results += f\"Content: {result.resume}\\n\\n\"\n",
    "\n",
    "        reference += f\"[{i+1}] - {result.title} ({result.url})\\n\"\n",
    "    \n",
    "    prompt = build_final_response.format(user_input=state.user_input, search_results=search_results)\n",
    "    print(\"\\n\\nFinal Writer Prompt:\\n\", prompt)\n",
    "\n",
    "    llm_result = llm.invoke(prompt)\n",
    "\n",
    "    final_response = llm_result + \"\\n\\nReferences:\\n\" + reference\n",
    "\n",
    "    return {\"final_response\": final_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "\n",
    "builder = StateGraph(ReportState)\n",
    "builder.add_node(\"build_first_queries\", build_first_queries)\n",
    "builder.add_node(\"single_search\", single_search)\n",
    "builder.add_node(\"final_writer\", final_writer)\n",
    "\n",
    "builder.add_edge(START, \"build_first_queries\")\n",
    "builder.add_conditional_edges(\"build_first_queries\", spawn_researchers, [\"single_search\"])\n",
    "builder.add_edge(\"single_search\", \"final_writer\")\n",
    "builder.add_edge(\"final_writer\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"user_input\": user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['final_response'].messages[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
